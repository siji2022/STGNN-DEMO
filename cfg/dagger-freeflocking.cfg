
[DEFAULT]

alg = dagger

continue_training = True
# learning parameters
batch_size = 64
buffer_size = 10000
updates_per_step = 200
seed = 12
actor_lr = 5e-5

n_train_episodes = 251
beta_coeff = 0.5
test_interval = 50
n_test_episodes = 20

# architecture parameters
hidden_size = 32
gamma = 0.8
tau = 0.5

# env parameters
env = FlockingLeader-v0
# env = FlockingRelative-v0
transfer_env_from = FlockingRelative-v0
comm_radius = 7.0
# n_agents = 200

n_agents = 20
n_leaders= 2

n_actions = 2
n_states = 6
debug = True
header = reward
dt = 0.01

max_accel=20
max_state_value=100
max_velocity=10

# ground truth control
c_alpha=10
c_beta=1
c_gamma=30

device=cuda:0
# device=cpu
# choose the ground truth methods: False is Saber's method
gt_centralized=True
# choose our model if use centralized or dencentralized
centralized=False

enable_alpha=True
enable_beta=False
enable_gamma=False

transfer_test=True

# [test1]
# test_gt=False
# k=1
# fname = dagger_k1

# [test2]
# test_gt=False
# k=2
# fname = dagger_k2

# [test3]
# test_gt=False
# k=3
# fname = dagger_k3


[test5]
test_gt=False
k=5
fname = dagger_k5
