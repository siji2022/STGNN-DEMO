from collections import deque

import gym
import io
from PIL import Image
from gym import spaces, error, utils
from gym.utils import seeding
from envs.flocking.saber_utils import *
from envs.flocking.utils import calc_metrics, calc_reward, constrant_initialization, plot_details
import numpy as np
import configparser
from os import path
import matplotlib.pyplot as plt
from matplotlib.pyplot import gca
from torch_geometric.utils import dense_to_sparse, add_remaining_self_loops
import torch

font = {'family': 'sans-serif',
        'weight': 'bold',
        'size': 14}


class FlockingRelativeSTEnv(gym.Env):
    metadata = {'render.modes': [
        'human', 'rgb_array'], 'video.frames_per_second': 50}

    def __init__(self):

        # groud truth generated by either centralized Tuner 2004 or Decentralized Saber 2004
        self.gt_centralized = False

        # if centralized, feed our model all neighbood information
        self.centralized = False

        # number states per agent
        self.nx_system = 4
        # numer of observations per agent
        self.n_features = 6
        # number of actions per agent
        self.nu = 2
        self.curr_step = 0
        # default problem parameters
        self.n_agents = 100  # int(config['network_size'])
        self.comm_radius = 0.9  # float(config['comm_radius'])

        self.dt = 0.01  # #float(config['system_dt'])

        self.comm_radius2 = self.comm_radius * self.comm_radius

        # intitialize state matrices
        self.x = None
        self.u = None
        self.u_gamma = None  # gamma action (n_agents,2)
        self.u_beta = None  # this is for controller to remove oscilliation; maybe a defeat in Saber, or my implementation is wrong
        self.obs_state = None
        self.mean_vel = None
        self.init_vel = None

        self.fig = None
        self.line1 = None

        # used to track the performance
        self.history = []
        self.u_history = []
        self.min_history = []
        self.max_history = []
        self.curr_step = 0

        # history states are hold here
        self.x_queue = deque()
        self.state_values_queue = deque()
        self.state_network_queue = deque()
        self.obs_values_queue = deque()
        self.obs_network_queue = deque()
        self.gamma_queue = deque()  # tracking target

        self.seed()

    def params_from_cfg(self, args):
        self.gt_centralized = args.getboolean('gt_centralized')
        self.centralized = args.getboolean('centralized')
        self.comm_radius = args.getfloat('comm_radius')
        self.comm_radius2 = self.comm_radius * self.comm_radius

        self.n_agents = args.getint('n_agents')
        self.NUMBER_OF_AGENTS = self.n_agents

        self.dt = args.getfloat('dt')
        self.len = args.getint('len')  # history length
        self.k = args.getint('k')  # k-hop delayed state
        # need the length of history to expand the delayed state and temporal series
        self.buffer_size = self.len+self.k-1

        # config similar to the Olfati-Saber's paper
        self.DISTANCE = self.comm_radius/1.2  # desire distance
        # r, interaction range, distance less than r will be neighbor; cant be larger than distance*1.414
        self.RANGE = self.comm_radius

        # controlls of the ground truth generators
        # control of neighbor distance
        self.C1_alpha = args.getfloat('c_alpha')
        self.C2_alpha = np.sqrt(self.C1_alpha)  # control of velocity

        self.max_accel = args.getfloat('max_accel')
        self.max_state_value = args.getfloat(
            'max_state_value')  # max state value
        self.max_velocity = args.getfloat('max_velocity')

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def step(self, u):
        assert u.shape == (self.n_agents, self.nu)
        u = np.clip(u, -self.max_accel, self.max_accel)
        self.u = u
        self.curr_step += 1

        # x position
        self.x[:, :2] += self.x[:, 2:]*self.dt
        self.x[:, :2] += self.u*self.dt*self.dt*0.5

        self.x[:, 2:] += self.u*self.dt
        self.x[:, 2:] = np.clip(
            self.x[:, 2:], -self.max_velocity, self.max_velocity)
        # self.obs_state = obs_projection_sphere(
        #     self.x, self.RK, self.yk)  # number_obs,number,4

        self.compute_helpers()

        # update history queue
        self.x_queue.append(self.x)
        self.x_queue.popleft()
        self.state_values_queue.append(torch.clone(self.state_values))
        self.state_values_queue.popleft()
        self.state_network_queue.append(torch.clone(self.state_network))
        self.state_network_queue.popleft()

        # saved in history for plot
        updated_state = np.copy(self.x)
        self.history += [updated_state]
        self.u_history += [self.u]
        end_eposode = False
        return (self.state_values, self.state_network), self.instant_cost(), end_eposode, {}

    def get_model_input(self):
        return self.state_values, self.state_network, \
            deque(self.state_values_queue), deque(self.state_network_queue), deque(
                self.obs_values_queue), deque(self.obs_network_queue), self.gamma_queue

    def compute_helpers(self):

        self.diff = self.x.reshape((self.n_agents, 1, self.nx_system)) - self.x.reshape(
            (1, self.n_agents, self.nx_system))
        self.r2 = np.multiply(self.diff[:, :, 0], self.diff[:, :, 0]) + np.multiply(self.diff[:, :, 1],
                                                                                    self.diff[:, :, 1])
        self.max_history += [np.sqrt(np.max(self.r2))]
        np.fill_diagonal(self.r2, np.Inf)
        self.min_history += [np.sqrt(np.min(self.r2))]

        self.x_features = np.dstack((self.diff[:, :, 2]/self.comm_radius, np.divide(self.diff[:, :, 0], np.multiply(self.r2, self.r2)*self.comm_radius), np.divide(self.diff[:, :, 0], self.r2),
                                     self.diff[:, :, 3]/self.comm_radius, np.divide(self.diff[:, :, 1], np.multiply(self.r2, self.r2)*self.comm_radius), np.divide(self.diff[:, :, 1], self.r2)))

        np.fill_diagonal(self.r2, 0)
        if not self.centralized:
            self.adj_mat = (self.r2 < self.comm_radius2).astype(float)
            self.state_network = self.adj_mat
        else:
            self.adj_mat = (self.r2 < np.Inf).astype(float)
            self.state_network = self.adj_mat

        np.fill_diagonal(self.r2, np.Inf)

        self.x_features = np.sum(self.x_features * self.adj_mat.reshape(
            self.n_agents, self.n_agents, 1), axis=1).reshape((self.n_agents, self.n_features))

        # convert into pytorch type
        self.state_network = torch.tensor(self.state_network)

        self.x_features = torch.clip(torch.tensor(
            self.x_features, dtype=torch.float), -self.max_state_value, self.max_state_value)
        self.state_values = self.x_features
        self.obs_state_values = None
        self.obs_state_network = None
        self.u_gamma = None

    def get_stats(self):

        stats = calc_metrics(self)
        return stats

    def instant_cost(self):  # sum of differences in velocities
        return calc_reward(self)

    def reset(self, n=None):
        if n is not None:
            self.n_agents = n
        self.fig = None
        self.history = []
        self.u_history = []
        self.min_history = []
        self.max_history = []
        # reset history queue
        self.x_queue = deque()
        self.state_values_queue = deque()
        self.state_network_queue = deque()
        self.obs_values_queue = deque()
        self.obs_network_queue = deque()
        self.gamma_queue = deque()
        self.curr_step = 0
        # initialization simple way, vel is 0
        x = np.hstack([np.random.uniform(0, np.sqrt(
            self.n_agents)*self.comm_radius, (self.n_agents, 2)), np.zeros((self.n_agents, 2))])

        constrant_initialization(self)

        self.compute_helpers()

        # populate previous states; initial to the len of k
        while len(self.x_queue) < self.buffer_size:
            self.x_queue.append(self.x)
            self.state_values_queue.append(self.x_features)
            self.state_network_queue.append(self.state_network)
            self.obs_values_queue.append(self.obs_state_values)
            self.obs_network_queue.append(self.obs_state_network)
            self.gamma_queue.append(self.u_gamma)

        self.x_init = np.copy(self.x)
        return (self.x_features, self.state_network)

    def controller(self):
        if self.gt_centralized:
            return self.controller_centralized()

        adjacency_matrix = get_adjacency_matrix(
            self.x, self.RANGE)  # N*N matrix with True/False.
        # print(get_adj_min_dist(multi_agent_system.agents))
        u = np.zeros((self.NUMBER_OF_AGENTS, 2))
        neighbors_p = self.x[:, :2].reshape(self.NUMBER_OF_AGENTS, 1, 2)
        neighbors_q = self.x[:, 2:].reshape(self.NUMBER_OF_AGENTS, 1, 2)
        agent_p = self.x[:, :2].reshape(1, self.NUMBER_OF_AGENTS, 2)
        agent_v = self.x[:, 2:].reshape(1, self.NUMBER_OF_AGENTS, 2)
        n_ij = get_n_ij(agent_p, neighbors_p)
        term1 = np.sum(phi_alpha(sigma_norm(neighbors_p - agent_p), self.RANGE, self.DISTANCE, 0.2)
                       * n_ij*adjacency_matrix.reshape(self.NUMBER_OF_AGENTS, self.NUMBER_OF_AGENTS, 1), axis=0)

        a_ij = get_a_ij(neighbors_p, agent_p, self.RANGE, H=0.2)
        term2 = self.C2_alpha * np.sum(a_ij * (neighbors_q - agent_v)*adjacency_matrix.reshape(
            self.NUMBER_OF_AGENTS, self.NUMBER_OF_AGENTS, 1), axis=0)
        u_alpha = self.C1_alpha*term1 + self.C2_alpha*term2
        u += u_alpha

        return np.clip(u, -self.max_accel, self.max_accel)

    def controller_centralized(self):
        """
        The controller for flocking from Turner 2003.
        Returns: the optimal action
        """
        # normalize based on comm_radius
        diff = self.diff/self.comm_radius
        r2 = self.r2/self.comm_radius2
        r2 = r2
        # TODO use the helper quantities here more?
        potentials = np.dstack((diff, self.potential_grad(
            diff[:, :, 0], r2), self.potential_grad(diff[:, :, 1], r2)))
        potentials = np.nan_to_num(potentials, nan=0.0)  # fill nan with 0

        p_sum = np.sum(potentials, axis=1).reshape(
            (self.n_agents, self.nx_system + 2))
        controls = np.hstack(((- p_sum[:, 4] - p_sum[:, 2]).reshape(
            (-1, 1)), (- p_sum[:, 3] - p_sum[:, 5]).reshape(-1, 1)))
        # controls+=self.controller_gamma()
        controls = np.clip(controls*self.comm_radius, -
                           self.max_accel, self.max_accel)

        return controls

    def potential_grad(self, pos_diff, r2):
        r2_2 = np.multiply(r2, r2)
        # r2_2=np.nan_to_num(r2_2,nan=np.Inf)
        grad = -1.0 * np.divide(pos_diff, r2_2
                                ) + 1 * np.divide(pos_diff, r2)
        grad[r2 > 1.0] = 0
        return grad*2.0

    def plot(self, j=0, fname='', dir='plots'):
        plot_details(self, j, fname, dir)

    def render(self, mode='human'):
        """
        Render the environment with agents as points in 2D space
        """
        if self.fig is None:
            plt.ion()
            fig = plt.figure()
            self.ax = fig.add_subplot(111)
            line1, = self.ax.plot(self.x[:, 0], self.x[:, 1],
                                  'bo', markersize=2)  # Returns a tuple of line objects, thus the comma
            self.ax.plot([0], [0], 'kx')
            # if self.quiver is None:

            self.quiver = self.ax.quiver(
                self.x[:, 0], self.x[:, 1], self.x[:, 2], self.x[:, 3], scale=10, scale_units='inches')

            plt.ylim(np.min(self.x[:, 1]) - 5, np.max(self.x[:, 1]) + 5)
            plt.xlim(np.min(self.x[:, 0]) - 5, np.max(self.x[:, 0]) + 5)
            plt.grid(which='both')

            a = gca()

            plt.title('GNN Controller {} agents'.format(self.n_agents))
            self.fig = fig
            self.line1 = line1

        self.line1.set_xdata(self.x[:, 0])
        self.line1.set_ydata(self.x[:, 1])

        plt.ylim(np.min(self.x[:, 1]) - 5, np.max(self.x[:, 1]) + 5)
        plt.xlim(np.min(self.x[:, 0]) - 5, np.max(self.x[:, 0]) + 5)
        a = gca()

        self.quiver.set_offsets(self.x[:, 0:2])
        self.quiver.set_UVC(self.x[:, 2], self.x[:, 3])

        if mode == 'human':
            self.fig.canvas.draw()
            self.fig.canvas.flush_events()
            return self.fig.canvas.draw()
        else:
            # test
            buf = io.BytesIO()
            self.fig.savefig(buf)
            buf.seek(0)
            im = np.asarray(Image.open(buf))
            # buf.close()
            return im

    def close(self):
        pass
